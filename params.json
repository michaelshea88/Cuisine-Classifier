{
  "name": "Text Mining with Recipe Data",
  "tagline": "Using recipe ingredients to categorize the cuisine",
  "body": "Websites like Allrecipes, Epicurious and Yummly aggregate millions of recipes from around the web and compile them into a centralized resource that allows users to search for and compare recipes. Some of the recipes pulled onto these sites come with lots of details, such as difficulty level, course, cuisine, total cost of ingredients, cook time, flavor profile, cuisine, etc. Others contain only a list of ingredients and instructions for preparation. \r\n\r\nSince extra details about recipes allow users to search and filter more effectively, many websites have developed algorithms to predict the characteristics of recipes that contain missing information. Below is an overview of the steps I took to replicate the process of using machine learning to infer unknown recipe attributes. \r\n\r\n# Step 1: Get the data\r\n![yum](http://www.yummly.com/wp-content/uploads/2013/03/Screen-Shot-2013-03-17-at-8.04.04-PM1.png)\r\n\r\nI obtained access to the [Yummly Recipe API](https://developer.yummly.com/), which contains data on over 2 million recipes. The API has very good documentation, and academic access can be requested if you're using it for educational purposes. It allows you to set a variety of search parameters, such as allowed_course, excluded_course, allowed_cuisine and excluded_cuisine. Although there was no apparent limit to the maximum number of responses you could request per API call, I found that batches of 500 worked best. Below is some of the code I used to make an API call. It uses the requests library, which returns the response in JSON format if the retrieval was successful. Once the JSON file was retrieved from the API I converted it to a Python dictionary which I then converted to a pandas DataFrame. \r\n\r\n# Step 2: Store on AWS\r\nSince I was only requesting 500 recipes at a time, I had to make a series of API calls over the course of about a week. I stored the data retrieved from each batch on a Postgres instance on Amazon's Relational Database Service. I used pandas' very handy to_sql function to add slices of data to the database I created. Then when I had all the data compiled in  Postgres, I used the read_sql function to make queries directly into pandas. \r\n\r\nHere's how I sent data to sql. The Postgres instance I created is no longer active.\r\n\r\n```python \r\nfrom sqlalchemy import create_engine\r\nimport pandas as pd\r\n\r\n#establish db connection\r\nengine = create_engine('postgresql://treytrey3:113315th3@recipeproject3.czcsc2tr7kct.us-east-1.rds.amazonaws.com:5432/dsicapstone3')\r\n\r\n#sample dataframe \r\ndf = pd.read_csv('../ingredients_combined/ingredients_reduced.csv')\r\n\r\n#name it and send to sql\r\nname = 'ingredients'\r\ndf.to_sql(name, engine, flavor='postgres', if_exists='replace')\r\n```\r\n\r\n# Step 3: Normalize text data\r\n\r\n```python\r\nfrom nltk.stem import WordNetLemmatizer\r\n```\r\nI used NLTK's [WordNetLemmatizer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.wordnet) to normalize ingredient names. Lemmatization is a natural language processing (NLP) technique that allows you to ignore trivial differences between different forms of the same word. Stanford's [Introduction to Information Retrieval](http://nlp.stanford.edu/IR-book/) is a good resource for learning about NLP. Here they discuss the difference between lemmatization and another technique called stemming:\r\n\r\n> Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\r\n\r\nFor example, compare the output of the word \"sausage\" when passed through a lemmatizer and a stemmer:\r\n\r\n```python\r\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\r\n\r\nwordnet_lemmatizer = WordNetLemmatizer()\r\nporter_stemmer = PorterStemmer()\r\n\r\nprint wordnet_lemmatizer.lemmatize('sausages')\r\nprint porter_stemmer.stem('sausages')\r\n```\r\n```python\r\nsausage\r\nsausag\r\n```\r\n\r\n# Step 4: Create Bag of Words\r\n```python\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\n```\r\n\r\nThe above [scikit-learn](scikit-learn.org) \"feature_extraction\" package was also really important to the success of this project. It contains many common NLP-related tools. One that was especially useful was the TfidfVectorizer. Tf-idf stands for *term frequency inverse document frequency*. It's not complicated; it essentially weights the importance of words based on their frequency in a document. \r\n\r\nSo if you were going through recipes as I was, ingredients like onions and salt and pepper aren't very indicative of the cuisine of the recipe. But jalapeno or soy sauce or herring is more important, but also more uncommon. Tf-idf weighs rarer terms more heavily than common terms. [Here is an excellent overview](http://planspace.org/20150524-tfidf_is_about_what_matters/) of the concept. \r\n\r\n# Step 5: Deploy Naive Bayes\r\nThe beautiful thing about Naive Bayes, well there are two beautiful things about Naive Bayes. First, it is highly effective at text classification tasks. Second, it doesn't require much tuning at all, unlike other models like logistic regression. \r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}