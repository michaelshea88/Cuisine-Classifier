<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Text Mining with Recipe Data by michaelshea88</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Text Mining with Recipe Data</h1>
      <h2 class="project-tagline">Using recipe ingredients to categorize the cuisine</h2>
      <a href="https://github.com/michaelshea88/Cuisine-Classifier" class="btn">View on GitHub</a>
      <a href="https://github.com/michaelshea88/Cuisine-Classifier/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/michaelshea88/Cuisine-Classifier/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <p>Websites like Allrecipes, Epicurious and Yummly aggregate millions of recipes from around the web and compile them into a centralized resource that allows users to search for and compare recipes. Some of the recipes pulled onto these sites come with lots of details, such as difficulty level, course, cuisine, total cost of ingredients, cook time, flavor profile, cuisine, etc. Others contain only a list of ingredients and instructions for preparation. </p>

<p>Since extra details about recipes allow users to search and filter more effectively, many websites have developed algorithms to predict the characteristics of recipes that contain missing information. Below is an overview of the steps I took to replicate the process of using machine learning to infer unknown recipe attributes. </p>

<h1>
<a id="step-1-get-the-data" class="anchor" href="#step-1-get-the-data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 1: Get the data</h1>

<p>I obtained access to the <a href="https://developer.yummly.com/">Yummly Recipe API</a>, which contains data on over 2 million recipes. The API has very good documentation, and academic access can be requested if you're using it for educational purposes. It allows you to set a variety of search parameters, such as allowed_course, excluded_course, allowed_cuisine and excluded_cuisine. Although there was no apparent limit to the maximum number of responses you could request per API call, I found that batches of 500 worked best. Below is some of the code I used to make an API call. It uses the requests library, which returns the response in JSON format if the retrieval was successful. Once the JSON file was retrieved from the API I converted it to a Python dictionary which I then converted to a pandas DataFrame. </p>

<h1>
<a id="step-2-store-on-aws" class="anchor" href="#step-2-store-on-aws" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 2: Store on AWS</h1>

<p>Since I was only requesting 500 recipes at a time, I had to make a series of API calls over the course of about a week. I stored the data retrieved from each batch on a Postgres instance on Amazon's Relational Database Service. I used pandas' very handy to_sql function to add slices of data to the database I created. Then when I had all the data compiled in  Postgres, I used the read_sql function to make queries directly into pandas. </p>

<p>Here's how I sent data to sql. The Postgres instance I created is no longer active.</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> sqlalchemy <span class="pl-k">import</span> create_engine
<span class="pl-k">import</span> pandas <span class="pl-k">as</span> pd

<span class="pl-c">#establish db connection</span>
engine <span class="pl-k">=</span> create_engine(<span class="pl-s"><span class="pl-pds">'</span>postgresql://treytrey3:113315th3@recipeproject3.czcsc2tr7kct.us-east-1.rds.amazonaws.com:5432/dsicapstone3<span class="pl-pds">'</span></span>)

<span class="pl-c">#sample dataframe </span>
df <span class="pl-k">=</span> pd.read_csv(<span class="pl-s"><span class="pl-pds">'</span>../ingredients_combined/ingredients_reduced.csv<span class="pl-pds">'</span></span>)

<span class="pl-c">#name it and send to sql</span>
name <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">'</span>ingredients<span class="pl-pds">'</span></span>
df.to_sql(name, engine, <span class="pl-v">flavor</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>postgres<span class="pl-pds">'</span></span>, <span class="pl-v">if_exists</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>replace<span class="pl-pds">'</span></span>)</pre></div>

<h1>
<a id="step-3-clean-text-data" class="anchor" href="#step-3-clean-text-data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 3: Clean text data</h1>
<p>I had to preprocess the text data before I could normalize it. This required the removal of non-alphanumeric characters, as well as converting the text from a string to a list of individual words without whitespace. For example, here's how I split the text into a list that I could then iterate through:</p>

<span class="pl-c">#s# Remove non-alpha characters </span>
name_string1 <span class="pl-k">=</span> [re<span style="color: #333333">.</span>sub(<span style="background-color: #fff0f0">&#39;[^A-Za-z]&#39;</span>, <span style="background-color: #fff0f0">&#39; &#39;</span>, z)<span style="color: #333333">.</span>strip()<span style="color: #333333">.</span>lower() <span style="color: #008800; font-weight: bold">for</span> z <span style="color: #000000; font-weight: bold">in</span> df[<span style="background-color: #fff0f0">&#39;id&#39;</span>]]
</pre></div>


<h1>
<a id="step-4-normalize-text-data" class="anchor" href="#step-3-normalize-text-data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 3: Normalize text data</h1>

<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> nltk.stem <span class="pl-k">import</span> WordNetLemmatizer</pre></div>

<p>I used NLTK's <a href="http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.wordnet">WordNetLemmatizer</a> to normalize ingredient names. Lemmatization is a natural language processing (NLP) technique that allows you to ignore trivial differences between different forms of the same word. Stanford's <a href="http://nlp.stanford.edu/IR-book/">Introduction to Information Retrieval</a> is a good resource for learning about NLP. Here they discuss the difference between lemmatization and another technique called stemming:</p>

<blockquote>
<p>Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.</p>
</blockquote>

<p>For example, compare the output of the word "sausage" when passed through a lemmatizer and a stemmer:</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> nltk.stem <span class="pl-k">import</span> WordNetLemmatizer, PorterStemmer

wordnet_lemmatizer <span class="pl-k">=</span> WordNetLemmatizer()
porter_stemmer <span class="pl-k">=</span> PorterStemmer()

<span class="pl-c1">print</span> wordnet_lemmatizer.lemmatize(<span class="pl-s"><span class="pl-pds">'</span>sausages<span class="pl-pds">'</span></span>)
<span class="pl-c1">print</span> porter_stemmer.stem(<span class="pl-s"><span class="pl-pds">'</span>sausages<span class="pl-pds">'</span></span>)</pre></div>

<div class="highlight highlight-source-python"><pre>sausage
sausag</pre></div>

<h1>
<a id="step-5-create-bag-of-words" class="anchor" href="#step-4-create-bag-of-words" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 4: Create Bag of Words</h1>

<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> sklearn.feature_extraction.text <span class="pl-k">import</span> TfidfVectorizer</pre></div>

<p>The above <a href="scikit-learn.org">scikit-learn</a> "feature_extraction" package was also really important to the success of this project. It contains many common NLP-related tools. One that was especially useful was the TfidfVectorizer. Tf-idf stands for <em>term frequency inverse document frequency</em>. It's not complicated; it essentially weights the importance of words based on their frequency in a document. </p>

<p>So if you were going through recipes as I was, ingredients like onions and salt and pepper aren't very indicative of the cuisine of the recipe. But jalapeno or soy sauce or herring is more important, but also more uncommon. Tf-idf weighs rarer terms more heavily than common terms. <a href="http://planspace.org/20150524-tfidf_is_about_what_matters/">Here is an excellent overview</a> of the concept. </p>

<h1>
<a id="step-6-deploy-naive-bayes" class="anchor" href="#step-5-deploy-naive-bayes" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 5: Deploy Naive Bayes</h1>

<p>Naive Bayes is highly effective at text classification tasks because it assumes independence between each pair of features. The Bag of Words representation also assumes independence between all the words in each document, acccounting for at least part of theis synergy. It also doesn't require much tuning at all, unlike other models like logistic regression. You mainly have to decide whether to run the Multinomial or Bernouilli  flavor of the model. Since Naive Bayes is quite fast compared to other models, there's often time to run both and see how they compare.</p>
<p>Below is the accuracy score I generated from a Bernouilli Naive Bayes. Bernouilli performed slightly better than Multinomial Naive Bayes. Sometimes it's unclear why one model performs better than another, so it's always good to compare a few. Scikit-Learn docs postulate that Bernoulli may perform better than Multinomial Naive Bayes on datasets with shorter documents. This certainly makes sense in the current case, where each document was a short list of ingredients.</p>
<h3>
<a id="Accuracy Score: 97%" class="anchor" href="#accuracy-score:-97%" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Accuracy Score: 97%</h1>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/michaelshea88/Cuisine-Classifier">Text Mining with Recipe Data</a> is maintained by <a href="https://github.com/michaelshea88">michaelshea88</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
